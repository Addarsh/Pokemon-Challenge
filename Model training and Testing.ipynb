{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "data = pd.read_csv(\"inputs/all_train_data.csv\")\n",
    "col_list = list(data.columns)\n",
    "ncolumns = len(col_list)\n",
    "\n",
    "test_col_list = col_list.copy()\n",
    "test_col_list.pop()\n",
    "testcolumns = len(test_col_list)\n",
    "\n",
    "#Define Base continuous features\n",
    "HP_1 = tf.feature_column.numeric_column(\"HP_1\")\n",
    "HP_2 = tf.feature_column.numeric_column(\"HP_2\")\n",
    "Attack_1 = tf.feature_column.numeric_column(\"Attack_1\")\n",
    "Attack_2 = tf.feature_column.numeric_column(\"Attack_2\")\n",
    "Defense_1 = tf.feature_column.numeric_column(\"Defense_1\")\n",
    "Defense_2 = tf.feature_column.numeric_column(\"Defense_2\")\n",
    "Sp_Atk_1 = tf.feature_column.numeric_column(\"Sp_Atk_1\")\n",
    "Sp_Atk_2 = tf.feature_column.numeric_column(\"Sp_Atk_2\")\n",
    "Sp_Def_1 = tf.feature_column.numeric_column(\"Sp_Def_1\")\n",
    "Sp_Def_2 = tf.feature_column.numeric_column(\"Sp_Def_2\")\n",
    "Speed_1 = tf.feature_column.numeric_column(\"Speed_1\")\n",
    "Speed_2 = tf.feature_column.numeric_column(\"Speed_2\")\n",
    "\n",
    "#Define base categorical features\n",
    "Type1_1 = tf.feature_column.categorical_column_with_vocabulary_list('Type_1_1',['Rock', 'Grass', 'Fairy', 'Fire', 'Bug', 'Psychic', 'Fighting',\n",
    "       'Water', 'Normal', 'Ground', 'Electric', 'Dark', 'Ice', 'Dragon',\n",
    "       'Steel', 'Ghost', 'Flying', 'Poison'])\n",
    "Type1_2 = tf.feature_column.categorical_column_with_vocabulary_list('Type_1_2',['Rock', 'Grass', 'Fairy', 'Fire', 'Bug', 'Psychic', 'Fighting',\n",
    "       'Water', 'Normal', 'Ground', 'Electric', 'Dark', 'Ice', 'Dragon',\n",
    "       'Steel', 'Ghost', 'Flying', 'Poison'])\n",
    "Type2_1 = tf.feature_column.categorical_column_with_vocabulary_list('Type_2_1',['Ground', 'Fighting', 'Flying', 'None', 'Water', 'Electric', 'Dark',\n",
    "       'Ice', 'Steel', 'Ghost', 'Rock', 'Fairy', 'Psychic', 'Poison',\n",
    "       'Dragon', 'Grass', 'Fire', 'Bug', 'Normal'])\n",
    "Type2_2 = tf.feature_column.categorical_column_with_vocabulary_list('Type_2_2',['Ground', 'Fighting', 'Flying', 'None', 'Water', 'Electric', 'Dark',\n",
    "       'Ice', 'Steel', 'Ghost', 'Rock', 'Fairy', 'Psychic', 'Poison',\n",
    "       'Dragon', 'Grass', 'Fire', 'Bug', 'Normal'])\n",
    "Generation_1 = tf.feature_column.categorical_column_with_vocabulary_list('Generation_1',[2, 5, 1, 3, 4, 6])\n",
    "Generation_2 = tf.feature_column.categorical_column_with_vocabulary_list('Generation_2',[2, 5, 1, 3, 4, 6])\n",
    "Legendary_1 = tf.feature_column.categorical_column_with_vocabulary_list('Legendary_1',[0, 1])\n",
    "Legendary_2 = tf.feature_column.categorical_column_with_vocabulary_list('Legendary_2',[0, 1])\n",
    "\n",
    "base_columns = [HP_1, HP_2, Attack_1, Attack_2, Defense_1, Defense_2, Sp_Atk_1, Sp_Atk_2, Sp_Def_1, Sp_Def_2, Speed_1, Speed_2,\n",
    "               Type1_1, Type1_2, Type2_1, Type2_2, Generation_1, Generation_2, Legendary_1, Legendary_2]\n",
    "deep_columns = [\n",
    "    HP_1, HP_2, Attack_1, Attack_2, Defense_1, Defense_2, Sp_Atk_1, Sp_Atk_2, Sp_Def_1, Sp_Def_2, Speed_1, Speed_2,\n",
    "    tf.feature_column.indicator_column(Type1_1),tf.feature_column.indicator_column(Type1_2),\n",
    "    tf.feature_column.indicator_column(Type2_1),tf.feature_column.indicator_column(Type2_2),\n",
    "    tf.feature_column.indicator_column(Generation_1), tf.feature_column.indicator_column(Generation_2),\n",
    "    tf.feature_column.indicator_column(Legendary_1), tf.feature_column.indicator_column(Legendary_2)\n",
    "]\n",
    "model_dir = \"model\"\n",
    "model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir, linear_feature_columns=base_columns,\n",
    "    dnn_feature_columns=deep_columns, dnn_hidden_units=[100, 50,25],\n",
    "    dnn_optimizer = tf.train.ProximalAdagradOptimizer(\n",
    "    learning_rate=0.1))\n",
    "\n",
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "    def parse_csv(value):\n",
    "        assert tf.gfile.Exists(data_file), ('%s not found. Please make sure you have either run data_download.py or '\n",
    "                                            'set both arguments --train_data and --test_data.' % data_file)\n",
    "        print (\"Parsing \", data_file)\n",
    "        records_defaults = [[1.0] for i in range(ncolumns)]\n",
    "        records_defaults[0] = ['']\n",
    "        records_defaults[1] = ['']\n",
    "        records_defaults[8] = [1]\n",
    "        records_defaults[9] = [1]\n",
    "        records_defaults[10] = ['']\n",
    "        records_defaults[11] = ['']\n",
    "        records_defaults[18] = [1]\n",
    "        records_defaults[19] = [1]\n",
    "        records_defaults[20] = [1]\n",
    "        columns = tf.decode_csv(value, record_defaults=records_defaults)\n",
    "        features = dict(zip(col_list, columns))\n",
    "        labels = features.pop(\"Output\")\n",
    "        return features, tf.equal(labels, 1)\n",
    "    # Extract lines from input files using the Dataset API.\n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "    \n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "\n",
    "#Model training\n",
    "#set hyper_params, batch_size and num_epochs\n",
    "batch_size = 500\n",
    "num_epochs = 100\n",
    "\n",
    "model.train(input_fn = lambda: input_fn(\"inputs/train_data.csv\", num_epochs, True, batch_size))\n",
    "\n",
    "#Evaluate model on training and validation set\n",
    "results = model.evaluate(input_fn=lambda: input_fn(\n",
    "   \"inputs/val_data.csv\" , 1, False, batch_size))\n",
    "for key in sorted(results):\n",
    "  print('%s: %s' % (key, results[key]))\n",
    "\n",
    "print (\"Results on training set\")\n",
    "\n",
    "results = model.evaluate(input_fn=lambda: input_fn(\n",
    "   \"inputs/train_data.csv\" , 1, False, batch_size))\n",
    "for key in sorted(results):\n",
    "  print('%s: %s' % (key, results[key]))\n",
    "\n",
    "\n",
    "#Model testing/prediction\n",
    "def test_input_fn(data_file, num_epochs, batch_size):\n",
    "    def parse_csv(value):\n",
    "        assert tf.gfile.Exists(data_file), ('%s not found. Please make sure you have either run data_download.py or '\n",
    "                                            'set both arguments --train_data and --test_data.' % data_file)\n",
    "        print (\"Parsing test file: \", data_file)\n",
    "        records_defaults = [[1.0] for i in range(testcolumns)] #Output column not present in test data, hence ncolumns-1\n",
    "        records_defaults[0] = ['']\n",
    "        records_defaults[1] = ['']\n",
    "        records_defaults[8] = [1]\n",
    "        records_defaults[9] = [1]\n",
    "        records_defaults[10] = ['']\n",
    "        records_defaults[11] = ['']\n",
    "        records_defaults[18] = [1]\n",
    "        records_defaults[19] = [1]\n",
    "        columns = tf.decode_csv(value, record_defaults=records_defaults)\n",
    "        features = dict(zip(test_col_list, columns))\n",
    "        return features\n",
    "    # Extract lines from input files using the Dataset API.\n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "    \n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features = iterator.get_next()\n",
    "    return features\n",
    "\n",
    "pred_values = model.predict(input_fn = lambda: test_input_fn(\"inputs/test_data.csv\",1,batch_size))\n",
    "\n",
    "#Read tests file\n",
    "tests_data = pd.read_csv(\"data/tests.csv\")\n",
    "tests_data = tests_data.assign(Prediction = tests_data.loc[:, \"First_pokemon\"])\n",
    "for i, p in enumerate(pred_values):\n",
    "    tests_data.loc[i, \"Prediction\"] = tests_data.loc[i, \"First_pokemon\"] if p[\"class_ids\"] == 0 else tests_data.loc[i, \"Second_pokemon\"]\n",
    "    \n",
    "tests_data.to_csv(\"outputs/predictions.csv\", index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
